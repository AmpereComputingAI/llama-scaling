services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.app
    image: ghcr.io/amperecomputingai/llama-scaling:app-0.1
    container_name: llama-scaling-app
    network_mode: host
    tty: true
    restart: unless-stopped

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    image: ghcr.io/amperecomputingai/llama-scaling:api-0.1
    container_name: llama-scaling-api
    network_mode: host
    tty: true
    restart: unless-stopped
